{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkaME4xBq0Qx"
   },
   "source": [
    "# **EfficientNetB7 AXIAL images**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7TLO_TCiUX-"
   },
   "source": [
    "## SET UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB7\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import layers, losses\n",
    "\n",
    "\n",
    "DATA_CN_AD_PATH = ''\n",
    "DATA_CN_MCI_AD_PATH = ''\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 224"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_datasets_CN_AD():\n",
    "  data_dir = pathlib.Path(DATA_CN_AD_PATH)\n",
    "\n",
    "  train_ds = keras.utils.image_dataset_from_directory(\n",
    "      directory = data_dir,\n",
    "      subset=\"training\",\n",
    "      batch_size=BATCH_SIZE,\n",
    "      image_size=(IMG_SIZE, IMG_SIZE),\n",
    "      validation_split=0.2,\n",
    "      seed=123,\n",
    "      shuffle=True)\n",
    "\n",
    "  validation_ds = keras.utils.image_dataset_from_directory(\n",
    "      directory=data_dir,\n",
    "      subset=\"validation\",\n",
    "      batch_size=BATCH_SIZE,\n",
    "      image_size=(IMG_SIZE, IMG_SIZE),\n",
    "      validation_split=0.2,\n",
    "      seed=123,\n",
    "      shuffle=True)\n",
    "  \n",
    "  val_batches = tf.data.experimental.cardinality(validation_ds)\n",
    "  test_dataset = validation_ds.take(val_batches // 5)\n",
    "  validation_ds = validation_ds.skip(val_batches // 5)\n",
    "\n",
    "  print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_ds))\n",
    "  print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))\n",
    "\n",
    "  return train_ds, validation_ds, test_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_datasets_CN_MCI_AD():\n",
    "  data_dir = pathlib.Path(DATA_CN_MCI_AD_PATH)\n",
    "\n",
    "  train_ds = keras.utils.image_dataset_from_directory(\n",
    "      directory = data_dir,\n",
    "      subset=\"training\",\n",
    "      batch_size=BATCH_SIZE,\n",
    "      image_size=(IMG_SIZE, IMG_SIZE),\n",
    "      validation_split=0.2,\n",
    "      seed=123,\n",
    "      shuffle=True,\n",
    "      label_mode='categorical'\n",
    "      )\n",
    "\n",
    "  validation_ds = keras.utils.image_dataset_from_directory(\n",
    "      directory=data_dir,\n",
    "      subset=\"validation\",\n",
    "      batch_size=BATCH_SIZE,\n",
    "      image_size=(IMG_SIZE, IMG_SIZE),\n",
    "      validation_split=0.2,\n",
    "      seed=123,\n",
    "      shuffle=True,\n",
    "      label_mode='categorical'\n",
    "      )\n",
    "  \n",
    "  val_batches = tf.data.experimental.cardinality(validation_ds)\n",
    "  test_dataset = validation_ds.take(val_batches // 5)\n",
    "  validation_ds = validation_ds.skip(val_batches // 5)\n",
    "\n",
    "  print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_ds))\n",
    "  print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))\n",
    "\n",
    "  return train_ds, validation_ds, test_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def improve_performance(train_ds, validation_ds, test_ds):\n",
    "  # Caching and Prefetching\n",
    "\n",
    "  AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "  train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "  validation_ds = validation_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "  test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "  return train_ds, validation_ds, test_ds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def showEvolution(acc, val_acc, loss, val_loss, title):\n",
    "  plt.figure(figsize=(8, 8))\n",
    "  plt.subplot(2, 1, 1)\n",
    "  plt.plot(acc, label='Training Accuracy')\n",
    "  plt.plot(val_acc, label='Validation Accuracy')\n",
    "  plt.legend(loc='lower right')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.ylim([min(plt.ylim()),1])\n",
    "  plt.title(title+' Training and Validation Accuracy')\n",
    "\n",
    "  plt.subplot(2, 1, 2)\n",
    "  plt.plot(loss, label='Training Loss')\n",
    "  plt.plot(val_loss, label='Validation Loss')\n",
    "  plt.legend(loc='upper right')\n",
    "  plt.ylabel('Cross Entropy')\n",
    "  plt.ylim([0,1.0])\n",
    "  plt.title(title+' Training and Validation Loss')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def showEvolutionFineTune(acc, val_acc, loss, val_loss, initial_epochs, title):\n",
    "  plt.style.use('bmh')\n",
    "  plt.style.use('dark_background')\n",
    "  plt.figure(figsize=(8, 8))\n",
    "  plt.subplot(2, 1, 1)\n",
    "  plt.plot(acc, 'fuchsia', label='Training Accuracy', )\n",
    "  plt.plot(val_acc, 'springgreen', label='Validation Accuracy')\n",
    "  plt.ylim([0.0, 1])\n",
    "  plt.plot([initial_epochs-1,initial_epochs-1],plt.ylim(), \n",
    "           'deepskyblue', label='Start Fine Tuning')\n",
    "  plt.legend(loc='lower right')\n",
    "  plt.title(title+'  Training and Validation Accuracy')\n",
    "\n",
    "  plt.subplot(2, 1, 2)\n",
    "  plt.plot(loss,'fuchsia',label='Training Loss')\n",
    "  plt.plot(val_loss, 'springgreen', label='Validation Loss')\n",
    "  plt.plot([initial_epochs-1,initial_epochs-1], plt.ylim(),\n",
    "           'deepskyblue', label='Start Fine Tuning')\n",
    "  plt.legend(loc='upper right')\n",
    "  plt.title(title+'  Training and Validation Loss')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TL Y FT (CN vs AD)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def CN_AD_model():\n",
    "  train_ds, validation_ds, test_ds = get_datasets_CN_AD()\n",
    "\n",
    "  global train_dataset_CN_AD\n",
    "  global validation_dataset_CN_AD\n",
    "  global test_dataset_CN_AD\n",
    "  global class_names_CN_AD\n",
    "\n",
    "  train_dataset_CN_AD = train_ds\n",
    "  validation_dataset_CN_AD = validation_ds\n",
    "  test_dataset_CN_AD = test_ds\n",
    "  class_names_CN_AD = train_ds.class_names\n",
    "  print(class_names_CN_AD)\n",
    "\n",
    "  train_ds, validation_ds, test_ds = improve_performance(train_ds, validation_ds, test_ds)\n",
    "\n",
    "\n",
    "  data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip('horizontal_and_vertical'),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "  ])\n",
    "  preprocess_input = tf.keras.applications.efficientnet.preprocess_input\n",
    "  \n",
    "  # crear el modelo base pre-entrenado:\n",
    "  # include_top=False permite la extracción de características eliminando la última capa densa\n",
    "  IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "  base_model = EfficientNetB7(\n",
    "      include_top=False,\n",
    "      weights='imagenet',\n",
    "      input_shape=IMG_SHAPE\n",
    "  )\n",
    "\n",
    "  image_batch, label_batch = next(iter(train_dataset_CN_AD))\n",
    "  feature_batch = base_model(image_batch)\n",
    "  print(feature_batch.shape)\n",
    "\n",
    "  # Extracción de características\n",
    "  # Congelar la base convolucional\n",
    "  base_model.trainable = False\n",
    "\n",
    " # Agregar un encabezado de clasificación\n",
    "  global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "  feature_batch_average = global_average_layer(feature_batch)\n",
    "  print(feature_batch_average.shape)\n",
    " \n",
    "  prediction_layer = tf.keras.layers.Dense(1)\n",
    "  prediction_batch = prediction_layer(feature_batch_average)\n",
    "  print(prediction_batch.shape)\n",
    "\n",
    "  inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "  x = data_augmentation(inputs)\n",
    "  x = preprocess_input(x)\n",
    "  x = base_model(x, training=False)\n",
    "  # Reconstruir la parte superior\n",
    "  x = global_average_layer(x)\n",
    "  x = tf.keras.layers.Dropout(0.3)(x)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  outputs = prediction_layer(x)\n",
    "  model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "  # Compilar el modelo\n",
    "  base_learning_rate = 0.0001\n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  model.summary()\n",
    "\n",
    "\n",
    "  print(len(model.trainable_variables))\n",
    "\n",
    "  initial_epochs = 10\n",
    "\n",
    "  loss0, accuracy0 = model.evaluate(validation_dataset_CN_AD)\n",
    "\n",
    "  print(\"initial loss: {:.2f}\".format(loss0))\n",
    "  print(\"initial accuracy: {:.2f}\".format(accuracy0))\n",
    "\n",
    "  history = model.fit(train_dataset_CN_AD,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=validation_dataset_CN_AD)\n",
    "  \n",
    "  acc = history.history['accuracy']\n",
    "  val_acc = history.history['val_accuracy']\n",
    "\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "\n",
    "  showEvolution(acc, val_acc, loss, val_loss, 'CN-AD')\n",
    "  \n",
    "  # FINE TUNE\n",
    "  # Descongelar las capas superiores del modelo\n",
    "  base_model.trainable = True\n",
    "\n",
    "  print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "  # Fine-tune desde esta capa en adelante\n",
    "  fine_tune_at = 20\n",
    "\n",
    "  # Congelar todas las capas previas a la capa `fine_tune_at`\n",
    "  for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "  # Recompilar el modelo\n",
    "  model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate/10),\n",
    "              metrics=['accuracy'])\n",
    "  \n",
    "  model.summary()\n",
    "\n",
    "  print(len(model.trainable_variables))\n",
    "\n",
    "  # Continuar entrenando al modelo\n",
    "  fine_tune_epochs = 10\n",
    "  total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "  history_fine = model.fit(train_dataset_CN_AD,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=history.epoch[-1],\n",
    "                         validation_data=validation_dataset_CN_AD)\n",
    "\n",
    "  acc += history_fine.history['accuracy']\n",
    "  val_acc += history_fine.history['val_accuracy']\n",
    "\n",
    "  loss += history_fine.history['loss']\n",
    "  val_loss += history_fine.history['val_loss']\n",
    "\n",
    "  showEvolutionFineTune(acc, val_acc, loss, val_loss, initial_epochs, \"CN-AD\")\n",
    "\n",
    "  return model, history_fine"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TL Y FT (CN vs MCI vs AD)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def CN_MCI_AD_model():\n",
    "  train_ds, validation_ds, test_ds = get_datasets_CN_MCI_AD()\n",
    "\n",
    "  global train_dataset_CN_MCI_AD\n",
    "  global validation_dataset_CN_MCI_AD\n",
    "  global test_dataset_CN_MCI_AD\n",
    "  global class_names_CN_MCI_AD\n",
    "\n",
    "  train_dataset_CN_MCI_AD = train_ds\n",
    "  validation_dataset_CN_MCI_AD = validation_ds\n",
    "  test_dataset_CN_MCI_AD = test_ds\n",
    "  class_names_CN_MCI_AD = train_ds.class_names\n",
    "  print(class_names_CN_MCI_AD)\n",
    "\n",
    "  train_ds, validation_ds, test_ds = improve_performance(train_ds, validation_ds, test_ds)\n",
    "\n",
    "\n",
    "  data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip('horizontal_and_vertical'),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "  ])\n",
    "  preprocess_input = tf.keras.applications.efficientnet.preprocess_input\n",
    "  \n",
    "  # crear el modelo base pre-entrenado:\n",
    "  # include_top=False permite la extracción de características eliminando la última capa densa\n",
    "  IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "  base_model = EfficientNetB7(\n",
    "      include_top=False,\n",
    "      weights='imagenet',\n",
    "      input_shape=IMG_SHAPE\n",
    "  )\n",
    "\n",
    "  image_batch, label_batch = next(iter(train_dataset_CN_MCI_AD))\n",
    "  feature_batch = base_model(image_batch)\n",
    "  print(feature_batch.shape)\n",
    "\n",
    "  # Extracción de características\n",
    "  # Congelar la base convolucional\n",
    "  base_model.trainable = False\n",
    "\n",
    " # Agregar un encabezado de clasificación\n",
    "  global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "  feature_batch_average = global_average_layer(feature_batch)\n",
    "  print(feature_batch_average.shape)\n",
    " \n",
    "  prediction_layer = tf.keras.layers.Dense(3, activation='softmax')\n",
    "  prediction_batch = prediction_layer(feature_batch_average)\n",
    "  print(prediction_batch.shape)\n",
    "\n",
    "  inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "  x = data_augmentation(inputs)\n",
    "  x = preprocess_input(x)\n",
    "  x = base_model(x, training=False)\n",
    "  # Reconstruir la parte superior\n",
    "  x = global_average_layer(x)\n",
    "  x = tf.keras.layers.Dropout(0.3)(x)\n",
    "  x = layers.BatchNormalization()(x)\n",
    "  outputs = prediction_layer(x)\n",
    "  model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "  # Compilar el modelo\n",
    "  base_learning_rate = 0.0001\n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  model.summary()\n",
    "\n",
    "\n",
    "  print(len(model.trainable_variables))\n",
    "\n",
    "  initial_epochs = 10\n",
    "\n",
    "  loss0, accuracy0 = model.evaluate(validation_dataset_CN_MCI_AD)\n",
    "\n",
    "  print(\"initial loss: {:.2f}\".format(loss0))\n",
    "  print(\"initial accuracy: {:.2f}\".format(accuracy0))\n",
    "\n",
    "  history = model.fit(train_dataset_CN_MCI_AD,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=validation_dataset_CN_MCI_AD)\n",
    "  \n",
    "  acc = history.history['accuracy']\n",
    "  val_acc = history.history['val_accuracy']\n",
    "\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "\n",
    "  showEvolution(acc, val_acc, loss, val_loss, \"CN-MCI-AD\")\n",
    "  \n",
    "  # FINE TUNE\n",
    "  # Descongelar las capas superiores del modelo\n",
    "  base_model.trainable = True\n",
    "\n",
    "  print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "  # Fine-tune desde esta capa en adelante\n",
    "  fine_tune_at = 20\n",
    "\n",
    "  # Congelar todas las capas previas a la capa `fine_tune_at`\n",
    "  for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "  # Recompilar el modelo\n",
    "  model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate/10),\n",
    "              metrics=['accuracy'])\n",
    "  \n",
    "  model.summary()\n",
    "\n",
    "  print(len(model.trainable_variables))\n",
    "\n",
    "  # Continuar entrenando al modelo\n",
    "  fine_tune_epochs = 10\n",
    "  total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "  history_fine = model.fit(train_dataset_CN_MCI_AD,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=history.epoch[-1],\n",
    "                         validation_data=validation_dataset_CN_MCI_AD)\n",
    "\n",
    "  acc += history_fine.history['accuracy']\n",
    "  val_acc += history_fine.history['val_accuracy']\n",
    "\n",
    "  loss += history_fine.history['loss']\n",
    "  val_loss += history_fine.history['val_loss']\n",
    "\n",
    "  showEvolutionFineTune(acc, val_acc, loss, val_loss, initial_epochs, \"CN-MCI-AD\")\n",
    "\n",
    "  return model, history_fine"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MAIN PROGRAM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CN_AD_model, CN_AD_history  = CN_AD_model()\n",
    "CN_MCI_AD_model, CN_MCI_AD_history  = CN_MCI_AD_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss, accuracy = CN_AD_model.evaluate(test_dataset_CN_AD)\n",
    "print('CN-AD Test accuracy :', accuracy)\n",
    "\n",
    "loss, accuracy = CN_MCI_AD_model.evaluate(test_dataset_CN_MCI_AD)\n",
    "print('AN-MCI-AD Test accuracy :', accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CN-AD "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image_batch_CN_AD, label_batch_CN_AD = test_dataset_CN_AD.as_numpy_iterator().next()\n",
    "predictions_CN_AD = CN_AD_model.predict_on_batch(image_batch_CN_AD).flatten()\n",
    "\n",
    "# Aplicar sigmoid ya que nuestro modelo devuelve logits\n",
    "predictions_CN_AD = tf.nn.sigmoid(predictions_CN_AD)\n",
    "predictions_CN_AD = tf.where(predictions_CN_AD < 0.5, 0, 1)\n",
    "\n",
    "print('Predictions:\\n', predictions_CN_AD.numpy())\n",
    "print('Labels:\\n', label_batch_CN_AD)\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for i in range(9):\n",
    "#   ax = plt.subplot(3, 3, i + 1)\n",
    "#   plt.imshow(image_batch_CN_AD[i].astype(\"uint8\"))\n",
    "#   plt.title(class_names_CN_AD[predictions_CN_AD[i]])\n",
    "#   plt.axis(\"off\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CN-MCI-AD "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image_batch_CN_MCI_AD, label_batch_CN_MCI_AD = test_dataset_CN_MCI_AD.as_numpy_iterator().next()\n",
    "predictions_CN_MCI_AD = CN_MCI_AD_model.predict_on_batch(image_batch_CN_MCI_AD)\n",
    "\n",
    "predictions_CN_MCI_AD = tf.nn.softmax(predictions_CN_MCI_AD)\n",
    "predictions_CN_MCI_AD = np.argmax(predictions_CN_MCI_AD, axis=1)\n",
    "\n",
    "print('Predictions:\\n', predictions_CN_MCI_AD)\n",
    "print('Labels:\\n', np.argmax(label_batch_CN_MCI_AD, axis=1))\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for i in range(9):\n",
    "#   ax = plt.subplot(3, 3, i + 1)\n",
    "#   plt.imshow(image_batch_CN_MCI_AD[i].astype(\"uint8\"))\n",
    "#   plt.title(class_names_CN_MCI_AD[predictions_CN_MCI_AD[i]])\n",
    "#   plt.axis(\"off\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "print(classification_report(label_batch_CN_AD, predictions_CN_AD, \n",
    "                            target_names=class_names_CN_AD))\n",
    "\n",
    "m_c = tf.math.confusion_matrix(\n",
    "  label_batch_CN_AD,\n",
    "  predictions_CN_AD\n",
    ")\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=m_c.numpy(),\n",
    "                              display_labels=class_names_CN_AD)\n",
    "plt.style.use('default')\n",
    "disp.plot()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "y_true = np.argmax(label_batch_CN_MCI_AD, axis=1)\n",
    "y_pred = predictions_CN_MCI_AD\n",
    "print(classification_report(y_true, y_pred, \n",
    "                            target_names=class_names_CN_MCI_AD))\n",
    "m_c = tf.math.confusion_matrix(\n",
    "  y_true,\n",
    "  y_pred\n",
    ")\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=m_c.numpy(),\n",
    "                              display_labels=class_names_CN_MCI_AD)\n",
    "plt.style.use('default')\n",
    "disp.plot()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bcf97707d32c454ebcdfa7d89d530277cb26338c3e9fda5d9e846ad80a379b04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}