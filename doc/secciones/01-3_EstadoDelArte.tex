%! Author = raquelmolire
%! Date = 1/10/2022

% Document
En este apartado se incluye la situación actual del uso de técnicas de DL para el diagnóstico del Alzheimer, es decir,
la información sobre las últimas actualizaciones e investigaciones ya disponibles al público sobre este tema.

En la última década, las técnicas de aprendizaje profundo han alcanzado una enorme popularidad en el ámbito del análisis
de imágenes médicas y como herramienta de diagnóstico y seguimiento de enfermedades.
Al mismo tiempo, ha incrementado el uso de técnicas de neuroimagen para el diagnóstico de la EA, por lo que la unión de
ambas técnicas, de aprendizaje profundo y de neuroimagen, ha demostrado ser una combinación ideal para realizar un
correcto diagnóstico de la enfermedad y poder predecir la evolución de la misma.

El número de artículos de investigación de la detección de la EA que se han publicado en los últimos años ha crecido
exponencialmente.
Recogiendo información sobre múltiples resultados con el uso de distintos biomarcadores o distintos modelos profundos.

La mayoría de los artículos publicados tratan el tema de la misma manera: planteando un nuevo modelo de aprendizaje
profundo y no llegando a explorar mejoras a otras alternativas o reflexionar sobre qué recursos son mejores, siendo el
resultado un cúmulo de estudios que no producen una conclusión concreta a qué métodos son más óptimos y por qué.
Existe un artículo de la universidad en Newcastle, Australia, que realiza una revisión de la literatura de más de 100
artículos y que detalla los hallazgos y tendencias, examinando biomarcadores y características útiles, técnicas de
preprocesamiento necesarias y diferentes métodos de tratamiento de neuroimágenes.
Este estudio se ha utilizado de referencia para este apartado.

\subsection{Biomarcadores que intervienen en la detección de la EA}\label{subsec:biomarcadores-estado-del-arte}
Para la detección de la EA las técnicas de neuroimagen no invasivas más utilizadas en los estudios son: MRI, fMRI
(Imagen por resonancia magnética funcional)  y PET.

La fMRI es un tipo especial de MRI que proporciona un conjunto de imágenes del flujo sanguíneo de ciertas partes del
cerebro y también son utilizadas para evaluar daños cerebrales.

De estas técnicas, MRI es el biomarcador más utilizado en la literatura debido a que ha demostrado un alto rendimiento.
A pesar de que varios estudios han demostrado que la MRI es más discriminatoria en comparación con la PET, otros estiman
que la MRI es igual de discriminatoria que la PET o ligeramente menos.

Además MRI es la técnica más frecuente en las pruebas médicas para la detección de la EA ya que proporciona información
precisa y el coste de su realización es menor.
Lo que sitúa a esta técnica como la mejor candidata para avanzar en la investigación del diagnóstico de la enfermedad.

\subsection{Técnicas de preprocesamiento de biomarcadores}\label{subsec:preprocesamiento-estado-del-arte}
La mayoría de los trabajos de investigación, en especial los de aprendizaje automático (ML), necesitan aplicar un
preprocesamiento a los datos antes de poder manejarlos.
Con el uso de técnicas de DL, algunos pasos de preprocesamiento no son tan cruciales, no obstante, en la mayoría de
casos, se realiza un preprocesamiento de los datos.
Las técnicas de preprocesamiento más usadas son:
\begin{itemize}
    \item Normalización de la intensidad.
    \item Registro de la imagen.
    \item Segmentación del tejido.
    \item Segmentación del cráneo.
    \item Corrección del movimiento.\\
\end{itemize}

\subsection{Técnicas de análisis de neuroimagen}\label{subsec:analisis-de neuroimagen-estado-del-arte}
Las técnicas de análisis o de extracción de características en neuroimagen se pueden dividir en cuatro categorías,
dependiendo de las características extraídas: basadas en vóxeles, en cortes, en parches y en regiones de interés (ROI).

La técnica basada en vóxeles, también llamada morfometría basada en Vóxel (VBM), es la más sencilla, se utiliza en MRI,
permite el análisis de lesiones focales cerebrales y en ella se realiza una división del cerebro en vóxeles que son la
unidad cúbica que compone un objeto tridimensional, el equivalente al píxel en un objeto bidimensional.
Esta técnica permite identificar las diferencias de concentración entre la sustancia gris y la sustancia blanca del
cerebro mediante la comparación vóxel a vóxel de los valores de intensidad de los mismos.

La técnica basada en cortes consiste en extraer, mediante cortes, imágenes bidimensionales con la mayor información
posible.
Como conclusión de los artículos se obtiene que la vista coronal engloba las tres regiones más importantes del cerebro
relacionadas con la EA: el hipocampo, la corteza y los ventrículos, y que los cortes centrales son los que incluyen
áreas que tienen características más relevantes para la clasificación.
Sin embargo, ya que el uso de las tres vistas de las neuroimágenes tridimensionales puede proporcionar características
útiles adicionales para la clasificación, algunos estudios tienen en cuenta todas las vistas de la imagen (sagital,
coronal y axial).

En la técnica basada en parches, pequeños sub-volúmenes de la imagen definidos como cubos tridimensionales conocidos
como parches se usan para extraer patrones relacionados con la enfermedad.
La diferencia entre un parche y un vóxel es que los vóxeles constituyen la mínima unidad procesable.

La técnica basada en ROI se centra en las partes del cerebro más afectadas en la fase temprana de la EA, en vez de
ocuparse de todo el cerebro.

Las conclusiones obtenidas en la literatura sobre las técnicas de análisis en neuroimagen son las siguientes:
\begin{itemize}
    \item Los métodos basados en ROI y en parches son más exactos, ya que son los métodos con los que se obtiene
    información más relevante;
    sin embargo, son también los métodos más costosos.
    \item Los métodos basados en parches son más precisos que los basados en vóxeles, pero la selección de los parches
    con más información de la imagen es una tarea compleja.
    \item El uso de cortes 2D como entrada en lugar de la imagen 3D en su totalidad evita la generación de millones de
    parámetros de entrenamiento e induce a la simplificación de las redes a costa de perder información.
    \item Al utilizar métodos basados en cortes, las vistas sagital y coronal resultan ser las más discriminatorias,
    aunque las vistas axiales son las más utilizadas.
    Hay estudios que afirman que no hay diferencias significativas entre planos.\\
\end{itemize}

\subsection{Modelos profundos más utilizados para obtener patrones relacionados con la EA}
\label{subsec:modelos-profundos-estado-del-arte}
Lamentablemente la mayoría de los estudios no publican su código fuente por lo que resulta difícil comparar de manera
imparcial los estudios entre sí.
Además normalmente los artículos se limitan a comparar precisiones finales y no a comparar distintos factores como el
coste computacional.

Las redes neuronales convolucionales (CNN) son las más usadas en el ámbito de la medicina porque han demostrado una
alta precisión en términos de clasificación de imágenes médicas.

Entre los modelos observados, los mejores resultados se encuentran entre las 3D-CNN y las 2D-CNN. Ambas opciones tienen
un buen rendimiento para la extracción de características, en el caso de las  arquitecturas de red neuronal de tipo
3D-CNN estas muestran ligeramente un mayor rendimiento ya que recogen información tridimensional y, por tanto, más
cantidad de información, sin embargo  las de tipo 2D-CNN son más fáciles de entrenar, requieren de un menor coste
computacional.

En la mayor parte de los estudios se entrena un modelo profundo desde cero, pero, esto puede ser ineficiente, porque
el proceso de entrenamiento requiere mucho tiempo y un gran conjunto de datos.
Mientras que los conjuntos de datos para la detección y clasificación de objetos genéricos cuentan con millones de
imágenes, los conjuntos de datos de neuroimagen disponibles suelen ser pequeños, de solo cientos de imágenes, lo que da
lugar a un sobreajuste.
El Transfer Learning (TL) es más rápido y alcanza mejores resultados que el entrenamiento desde cero, de modo que,
generalmente resulta útil utilizar CNNs probadas y pre-entrenadas para volver a entrenarlas con otro conjunto de datos.

\subsection{Conjuntos de datos y herramientas de software útiles}\label{subsec:datos-y-herramientas-estado-del-arte}
Como paquetes software de análisis de neuroimagen están: FreeSurfer, FSL, MIPAV y SPM, que ofrecen herramientas potentes
para diversas técnicas de preprocesamiento automatizado comentadas en el
apartado~\ref{subsec:preprocesamiento-estado-del-arte}.

Para implementar modelos profundos las herramientas más utilizadas son: MATLAB, Keras, TensorFlow, Theano, Caffe, y
Torch.
Siendo Keras y TensorFlow los más utilizados y Torch el menos utilizado.

La principal dificultad a la que se enfrentan los investigadores es la falta de imágenes que permitan formar un gran
conjunto de datos de entrenamiento con el que poder obtener un resultado preciso y por tanto un diagnóstico fiable.
La escasez de datos es un problema que se debe a las restricciones de privacidad de las imágenes médicas y de su elevado
coste en muchos casos.

Como bases de datos online para obtener conjuntos de datos relativos a la EA, las más importantes son: ADNI, AIBL,
OASIS y MIRIAD, que ofrecen al público biomarcadores como neuroimagen, información genética y sanguínea, y evaluaciones
clínicas y cognitivas.
ADNI es la que más destaca de todas ellas por ser un estudio multicéntrico y longitudinal, además es el más común en la
literatura, de manera individual o en combinación con otros.
Esta última es una forma de poner solución a la insuficiencia de datos.
Al combinar diferentes conjuntos de datos, se produce una mayor heterogeneidad, lo cual es una desventaja, pero se crea
un modelo amplio para la clasificación y la predicción.

Las técnicas de aumento de datos entre las que se encuentran la reflexión, la rotación, la traslación aleatoria, el
desenfoque o el escalado mejoran el rendimiento de la clasificación sin la necesidad de recoger nuevos datos cuando
estos son limitados.

Se recomienda el uso de un conjunto de datos equilibrado, ya que, en caso contrario, un desequilibrio entre pacientes de
distintas categorías provoca una variación en la precisión.
Siendo, por tanto, preferible el uso de un conjunto de datos menor antes que uno desequilibrado.
